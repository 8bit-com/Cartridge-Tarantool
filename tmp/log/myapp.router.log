2023-05-01 13:13:21.371 [6781] main/103/init.lua I> Using advertise_uri "localhost:3301"
2023-05-01 13:13:21.371 [6781] main/103/init.lua I> Membership encryption enabled
2023-05-01 13:13:21.379 [6781] main/103/init.lua I> Probe uri was successful
2023-05-01 13:13:21.380 [6781] main/103/init.lua I> Membership BROADCAST sent to 192.168.1.255:3302
2023-05-01 13:13:21.381 [6781] main/103/init.lua I> Membership BROADCAST sent to 172.29.79.255:3302
2023-05-01 13:13:21.381 [6781] main/103/init.lua I> Membership BROADCAST sent to 192.168.56.255:3302
2023-05-01 13:13:21.382 [6781] main/103/init.lua I> Membership BROADCAST sent to 127.0.0.1:3302
2023-05-01 13:13:21.383 [6781] main/103/init.lua I> Membership BROADCAST sent to 172.30.15.255:3302
2023-05-01 13:13:21.384 [6781] main/103/init.lua I> Membership BROADCAST sent to 192.168.1.255:3301
2023-05-01 13:13:21.385 [6781] main/103/init.lua I> Membership BROADCAST sent to 172.29.79.255:3301
2023-05-01 13:13:21.386 [6781] main/103/init.lua I> Membership BROADCAST sent to 192.168.56.255:3301
2023-05-01 13:13:21.387 [6781] main/103/init.lua I> Membership BROADCAST sent to 127.0.0.1:3301
2023-05-01 13:13:21.388 [6781] main/103/init.lua I> Membership BROADCAST sent to 172.30.15.255:3301
2023-05-01 13:13:21.389 [6781] main/103/init.lua I> Membership BROADCAST sent to 192.168.1.255:3300
2023-05-01 13:13:21.389 [6781] main/103/init.lua I> Membership BROADCAST sent to 172.29.79.255:3300
2023-05-01 13:13:21.390 [6781] main/103/init.lua I> Membership BROADCAST sent to 192.168.56.255:3300
2023-05-01 13:13:21.391 [6781] main/103/init.lua I> Membership BROADCAST sent to 127.0.0.1:3300
2023-05-01 13:13:21.392 [6781] main/103/init.lua I> Membership BROADCAST sent to 172.30.15.255:3300
2023-05-01 13:13:21.393 [6781] main/107/http/0.0.0.0:8081 I> started
2023-05-01 13:13:21.394 [6781] main/103/init.lua I> Listening HTTP on 0.0.0.0:8081
2023-05-01 13:13:21.394 [6781] main/103/init.lua I> tcp_server: remove dead UNIX socket: /home/user/myapp/tmp/run/myapp.router.control
2023-05-01 13:13:21.395 [6781] main/108/console/unix/:/home/user/myapp/tmp/run/myapp.router.control I> started
2023-05-01 13:13:21.395 [6781] main/103/init.lua I> Load modules and register
2023-05-01 13:13:21.439 [6781] main/103/init.lua I> "tuple.keydef" module is not found. Built-in "key_def" is used
2023-05-01 13:13:21.444 [6781] main/103/init.lua I> "tuple.merger" module is not found. Built-in "merger" is used
2023-05-01 13:13:21.453 [6781] main/109/remote_control/127.0.0.1:3301 I> started
2023-05-01 13:13:21.453 [6781] main/103/init.lua I> Remote control bound to 127.0.0.1:3301
2023-05-01 13:13:21.453 [6781] main/103/init.lua I> Instance state changed:  -> ConfigFound
2023-05-01 13:13:21.465 [6781] main/103/init.lua I> Validate roles configurations
2023-05-01 13:13:21.465 [6781] main/103/init.lua I> Validate config "ddl-manager" role
2023-05-01 13:13:21.465 [6781] main/103/init.lua I> Successfully validated config "ddl-manager" role in 0.000011 sec
2023-05-01 13:13:21.465 [6781] main/103/init.lua I> Validate config "vshard-router" role
2023-05-01 13:13:21.465 [6781] main/103/init.lua I> Successfully validated config "vshard-router" role in 0.000077 sec
2023-05-01 13:13:21.465 [6781] main/103/init.lua I> Validate config "metrics" role
2023-05-01 13:13:21.465 [6781] main/103/init.lua I> Successfully validated config "metrics" role in 0.000002 sec
2023-05-01 13:13:21.465 [6781] main/103/init.lua I> Validate config "crud-router" role
2023-05-01 13:13:21.465 [6781] main/103/init.lua I> Successfully validated config "crud-router" role in 0.000001 sec
2023-05-01 13:13:21.465 [6781] main/103/init.lua I> Roles configuration validation finished
2023-05-01 13:13:21.465 [6781] main/103/init.lua I> Instance state changed: ConfigFound -> ConfigLoaded
2023-05-01 13:13:21.466 [6781] main/110/lua I> Instance state changed: ConfigLoaded -> RecoveringSnapshot
2023-05-01 13:13:21.466 [6781] main/110/lua confapplier.lua:549 W> Calling box.cfg()...
2023-05-01 13:13:21.472 [6781] main/110/lua I> Tarantool 2.10.6-0-g3990f976b Linux-x86_64-RelWithDebInfo
2023-05-01 13:13:21.472 [6781] main/110/lua I> log level 5
2023-05-01 13:13:21.472 [6781] main/110/lua I> wal/engine cleanup is paused
2023-05-01 13:13:21.473 [6781] main/110/lua I> mapping 268435456 bytes for memtx tuple arena...
2023-05-01 13:13:21.473 [6781] main/110/lua I> Actual slab_alloc_factor calculated on the basis of desired slab_alloc_factor = 1.044274
2023-05-01 13:13:21.473 [6781] main/110/lua I> mapping 134217728 bytes for vinyl tuple arena...
2023-05-01 13:13:21.473 [6781] main/110/lua I> Recovering snapshot with schema version 2.10.5
2023-05-01 13:13:21.474 [6781] main/110/lua systemd.c:134 !> systemd: failed to send message: Transport endpoint is not connected
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> Cartridge 2.7.9
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> server alias router
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> advertise uri localhost:3301
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> working directory /home/user/myapp/tmp/data/myapp.router
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> Cartridge options:
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> advertise_uri = localhost:3301
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> alias = router
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> auth_backend_name = cartridge.auth-backend
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> auth_enabled = nil
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> bucket_count = nil
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> console_sock = /home/user/myapp/tmp/run/myapp.router.control
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> http_enabled = true
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> http_host = 0.0.0.0
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> http_port = 8081
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> roles = ["cartridge.roles.vshard-storage","cartridge.roles.vshard-router","cartridge.roles.metrics","app.roles.api","app.roles.repo_api","app.roles.storage","app.roles.myqueue","cartridge.roles.crud-router","cartridge.roles.crud-storage"]
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> roles_reload_allowed = nil
2023-05-01 13:13:21.474 [6781] main/103/init.lua I> swim_broadcast = true
2023-05-01 13:13:21.475 [6781] main/103/init.lua I> upgrade_schema = nil
2023-05-01 13:13:21.475 [6781] main/103/init.lua I> upload_prefix = nil
2023-05-01 13:13:21.475 [6781] main/103/init.lua I> vshard_groups = nil
2023-05-01 13:13:21.475 [6781] main/103/init.lua I> webui_blacklist = nil
2023-05-01 13:13:21.475 [6781] main/103/init.lua I> webui_enabled = true
2023-05-01 13:13:21.475 [6781] main/103/init.lua I> webui_enforce_root_redirect = true
2023-05-01 13:13:21.475 [6781] main/103/init.lua I> webui_prefix = 
2023-05-01 13:13:21.475 [6781] main/103/init.lua I> workdir = /home/user/myapp/tmp/data/myapp.router
2023-05-01 13:13:21.475 [6781] main/103/init.lua cartridge.lua:910 W> In the next releases validation of instances labels will be more strict. Make sure you don't use invalid labels. For details, see https://github.com/tarantool/cartridge/pull/1980
2023-05-01 13:13:21.475 [6781] main/103/init.lua I> Set default metrics endpoints
2023-05-01 13:13:21.475 [6781] main/111/lua I> Cartridge 2.7.9
2023-05-01 13:13:21.475 [6781] main/111/lua I> server alias router
2023-05-01 13:13:21.475 [6781] main/111/lua I> advertise uri localhost:3301
2023-05-01 13:13:21.475 [6781] main/111/lua I> working directory /home/user/myapp/tmp/data/myapp.router
2023-05-01 13:13:21.476 [6781] main/110/lua I> update replication_synchro_quorum = 1
2023-05-01 13:13:21.476 [6781] main/110/lua I> instance uuid 8a1e23de-f0e4-43bd-8c9d-8933177a48cc
2023-05-01 13:13:21.484 [6781] main I> entering the event loop
2023-05-01 13:13:21.484 [6781] main systemd.c:134 !> systemd: failed to send message: Transport endpoint is not connected
2023-05-01 13:13:21.485 [6781] main/110/lua I> instance vclock {0: 1, 1: 23}
2023-05-01 13:13:21.486 [6781] main/110/lua I> recovery start
2023-05-01 13:13:21.486 [6781] main/110/lua I> recovering from `/home/user/myapp/tmp/data/myapp.router/00000000000000000000.snap'
2023-05-01 13:13:21.486 [6781] main/110/lua I> cluster uuid 23dbaca1-c88a-4fd5-953c-938a7c7c03d5
2023-05-01 13:13:21.499 [6781] main/110/lua I> assigned id 1 to replica 8a1e23de-f0e4-43bd-8c9d-8933177a48cc
2023-05-01 13:13:21.499 [6781] main/110/lua I> update replication_synchro_quorum = 1
2023-05-01 13:13:21.499 [6781] main/110/lua I> recover from `/home/user/myapp/tmp/data/myapp.router/00000000000000000000.xlog'
2023-05-01 13:13:21.500 [6781] main/110/lua I> done `/home/user/myapp/tmp/data/myapp.router/00000000000000000000.xlog'
2023-05-01 13:13:21.500 [6781] main/110/lua I> recover from `/home/user/myapp/tmp/data/myapp.router/00000000000000000024.xlog'
2023-05-01 13:13:21.500 [6781] main/110/lua I> done `/home/user/myapp/tmp/data/myapp.router/00000000000000000024.xlog'
2023-05-01 13:13:21.501 [6781] main/110/lua I> RAFT: fencing enabled
2023-05-01 13:13:21.501 [6781] main/110/lua systemd.c:134 !> systemd: failed to send message: Transport endpoint is not connected
2023-05-01 13:13:21.501 [6781] main/110/lua I> ready to accept requests
2023-05-01 13:13:21.501 [6781] main/110/lua I> leaving orphan mode
2023-05-01 13:13:21.501 [6781] main/110/lua systemd.c:134 !> systemd: failed to send message: Transport endpoint is not connected
2023-05-01 13:13:21.501 [6781] main/112/gc I> wal/engine cleanup is resumed
2023-05-01 13:13:21.501 [6781] main/110/lua I> set 'custom_proc_title' configuration option to "myapp@router"
2023-05-01 13:13:21.501 [6781] main/110/lua I> set 'log_level' configuration option to 5
2023-05-01 13:13:21.501 [6781] main/113/checkpoint_daemon I> scheduled next checkpoint for Mon May  1 14:44:22 2023
2023-05-01 13:13:21.514 [6781] main/110/lua I> set 'log_format' configuration option to "plain"
2023-05-01 13:13:21.516 [6781] main/110/lua I> set 'replication_connect_quorum' configuration option to 100
2023-05-01 13:13:21.516 [6781] main/110/lua I> set 'read_only' configuration option to true
2023-05-01 13:13:21.518 [6781] main/110/lua I> Making sure user "admin" exists...
2023-05-01 13:13:21.518 [6781] main/110/lua I> Remote control stopped
2023-05-01 13:13:21.518 [6781] main/109/remote_control/127.0.0.1:3301 I> stopped
2023-05-01 13:13:21.518 [6781] main/110/lua I> tx_binary: bound to 127.0.0.1:3301
2023-05-01 13:13:21.521 [6781] main/110/lua I> set 'listen' configuration option to "127.0.0.1:3301"
2023-05-01 13:13:21.521 [6781] main/110/lua I> Instance state changed: RecoveringSnapshot -> ConnectingFullmesh
2023-05-01 13:13:21.521 [6781] main/110/lua I> connecting to 1 replicas
2023-05-01 13:13:21.522 [6781] main/126/applier/admin@localhost:3301 I> remote master 8a1e23de-f0e4-43bd-8c9d-8933177a48cc at 127.0.0.1:3301 running Tarantool 2.10.6
2023-05-01 13:13:21.522 [6781] main/110/lua I> connected to 1 replicas
2023-05-01 13:13:21.522 [6781] main/110/lua I> synchronizing with 1 replicas
2023-05-01 13:13:21.522 [6781] main/126/applier/admin@localhost:3301 I> leaving orphan mode
2023-05-01 13:13:21.522 [6781] main/126/applier/admin@localhost:3301 systemd.c:134 !> systemd: failed to send message: Transport endpoint is not connected
2023-05-01 13:13:21.522 [6781] main/110/lua I> replica set sync complete
2023-05-01 13:13:21.522 [6781] main/110/lua I> leaving orphan mode
2023-05-01 13:13:21.522 [6781] main/110/lua systemd.c:134 !> systemd: failed to send message: Transport endpoint is not connected
2023-05-01 13:13:21.522 [6781] main/110/lua I> set 'replication' configuration option to ["admin@localhost:3301"]
2023-05-01 13:13:21.522 [6781] main/110/lua I> Instance state changed: ConnectingFullmesh -> BoxConfigured
2023-05-01 13:13:21.522 [6781] main/110/lua I> Tarantool options:
2023-05-01 13:13:21.522 [6781] main/110/lua I> audit_log = nil
2023-05-01 13:13:21.522 [6781] main/110/lua I> audit_nonblock = true
2023-05-01 13:13:21.522 [6781] main/110/lua I> audit_format = json
2023-05-01 13:13:21.522 [6781] main/110/lua I> audit_filter = compatibility
2023-05-01 13:13:21.522 [6781] main/110/lua I> background = false
2023-05-01 13:13:21.522 [6781] main/110/lua I> checkpoint_count = 2
2023-05-01 13:13:21.522 [6781] main/110/lua I> checkpoint_interval = 3600
2023-05-01 13:13:21.522 [6781] main/110/lua I> checkpoint_wal_threshold = 1e+18
2023-05-01 13:13:21.522 [6781] main/110/lua I> custom_proc_title = myapp@router
2023-05-01 13:13:21.522 [6781] main/110/lua I> election_fencing_mode = nil
2023-05-01 13:13:21.522 [6781] main/110/lua I> election_mode = off
2023-05-01 13:13:21.522 [6781] main/110/lua I> election_timeout = 5
2023-05-01 13:13:21.523 [6781] main/110/lua I> feedback_enabled = true
2023-05-01 13:13:21.523 [6781] main/110/lua I> feedback_host = https://feedback.tarantool.io
2023-05-01 13:13:21.523 [6781] main/110/lua I> feedback_interval = 3600
2023-05-01 13:13:21.523 [6781] main/110/lua I> feedback_crashinfo = true
2023-05-01 13:13:21.523 [6781] main/110/lua I> flightrec_enabled = false
2023-05-01 13:13:21.523 [6781] main/110/lua I> flightrec_logs_size = 10485760
2023-05-01 13:13:21.523 [6781] main/110/lua I> flightrec_logs_max_msg_size = 4096
2023-05-01 13:13:21.523 [6781] main/110/lua I> flightrec_logs_log_level = 6
2023-05-01 13:13:21.523 [6781] main/110/lua I> flightrec_metrics_interval = 1
2023-05-01 13:13:21.523 [6781] main/110/lua I> flightrec_metrics_period = 180
2023-05-01 13:13:21.523 [6781] main/110/lua I> flightrec_requests_size = 10485760
2023-05-01 13:13:21.523 [6781] main/110/lua I> flightrec_requests_max_req_size = 16384
2023-05-01 13:13:21.523 [6781] main/110/lua I> flightrec_requests_max_res_size = 16384
2023-05-01 13:13:21.523 [6781] main/110/lua I> force_recovery = false
2023-05-01 13:13:21.523 [6781] main/110/lua I> hot_standby = false
2023-05-01 13:13:21.523 [6781] main/110/lua I> instance_uuid = nil
2023-05-01 13:13:21.523 [6781] main/110/lua I> io_collect_interval = nil
2023-05-01 13:13:21.523 [6781] main/110/lua I> iproto_threads = 1
2023-05-01 13:13:21.523 [6781] main/110/lua I> listen = 127.0.0.1:3301
2023-05-01 13:13:21.523 [6781] main/110/lua I> log = nil
2023-05-01 13:13:21.523 [6781] main/110/lua I> log_format = plain
2023-05-01 13:13:21.523 [6781] main/110/lua I> log_level = 5
2023-05-01 13:13:21.523 [6781] main/110/lua I> log_nonblock = nil
2023-05-01 13:13:21.523 [6781] main/110/lua I> memtx_dir = /home/user/myapp/tmp/data/myapp.router
2023-05-01 13:13:21.523 [6781] main/110/lua I> memtx_max_tuple_size = 1048576
2023-05-01 13:13:21.523 [6781] main/110/lua I> memtx_memory = 268435456
2023-05-01 13:13:21.523 [6781] main/110/lua I> memtx_allocator = small
2023-05-01 13:13:21.523 [6781] main/110/lua I> memtx_min_tuple_size = 16
2023-05-01 13:13:21.523 [6781] main/110/lua I> memtx_use_mvcc_engine = false
2023-05-01 13:13:21.523 [6781] main/110/lua I> net_msg_max = 768
2023-05-01 13:13:21.523 [6781] main/110/lua I> pid_file = /home/user/myapp/tmp/run/myapp.router.pid
2023-05-01 13:13:21.523 [6781] main/110/lua I> read_only = true
2023-05-01 13:13:21.523 [6781] main/110/lua I> readahead = 16320
2023-05-01 13:13:21.523 [6781] main/110/lua I> replicaset_uuid = nil
2023-05-01 13:13:21.523 [6781] main/110/lua I> replication = table: 0x7f6dbea8
2023-05-01 13:13:21.523 [6781] main/110/lua I> replication_anon = false
2023-05-01 13:13:21.523 [6781] main/110/lua I> replication_connect_quorum = 100
2023-05-01 13:13:21.523 [6781] main/110/lua I> replication_connect_timeout = 30
2023-05-01 13:13:21.523 [6781] main/110/lua I> replication_skip_conflict = false
2023-05-01 13:13:21.523 [6781] main/110/lua I> replication_sync_lag = 10
2023-05-01 13:13:21.523 [6781] main/110/lua I> replication_sync_timeout = 300
2023-05-01 13:13:21.523 [6781] main/110/lua I> replication_synchro_quorum = N / 2 + 1
2023-05-01 13:13:21.523 [6781] main/110/lua I> replication_synchro_timeout = 5
2023-05-01 13:13:21.523 [6781] main/110/lua I> replication_timeout = 1
2023-05-01 13:13:21.523 [6781] main/110/lua I> replication_threads = 1
2023-05-01 13:13:21.523 [6781] main/110/lua I> slab_alloc_factor = 1.05
2023-05-01 13:13:21.523 [6781] main/110/lua I> slab_alloc_granularity = 8
2023-05-01 13:13:21.523 [6781] main/110/lua I> snap_io_rate_limit = nil
2023-05-01 13:13:21.523 [6781] main/110/lua I> sql_cache_size = 5242880
2023-05-01 13:13:21.523 [6781] main/110/lua I> strip_core = true
2023-05-01 13:13:21.523 [6781] main/110/lua I> txn_isolation = best-effort
2023-05-01 13:13:21.523 [6781] main/110/lua I> too_long_threshold = 0.5
2023-05-01 13:13:21.523 [6781] main/110/lua I> username = nil
2023-05-01 13:13:21.523 [6781] main/110/lua I> vinyl_bloom_fpr = 0.05
2023-05-01 13:13:21.523 [6781] main/110/lua I> vinyl_cache = 134217728
2023-05-01 13:13:21.523 [6781] main/110/lua I> vinyl_defer_deletes = false
2023-05-01 13:13:21.523 [6781] main/110/lua I> vinyl_dir = /home/user/myapp/tmp/data/myapp.router
2023-05-01 13:13:21.523 [6781] main/110/lua I> vinyl_max_tuple_size = 1048576
2023-05-01 13:13:21.523 [6781] main/110/lua I> vinyl_memory = 134217728
2023-05-01 13:13:21.523 [6781] main/110/lua I> vinyl_page_size = 8192
2023-05-01 13:13:21.523 [6781] main/110/lua I> vinyl_range_size = nil
2023-05-01 13:13:21.523 [6781] main/110/lua I> vinyl_read_threads = 1
2023-05-01 13:13:21.523 [6781] main/110/lua I> vinyl_run_count_per_level = 2
2023-05-01 13:13:21.523 [6781] main/110/lua I> vinyl_run_size_ratio = 3.5
2023-05-01 13:13:21.523 [6781] main/110/lua I> vinyl_timeout = 60
2023-05-01 13:13:21.523 [6781] main/110/lua I> vinyl_write_threads = 4
2023-05-01 13:13:21.523 [6781] main/110/lua I> wal_dir = /home/user/myapp/tmp/data/myapp.router
2023-05-01 13:13:21.524 [6781] main/110/lua I> wal_dir_rescan_delay = 2
2023-05-01 13:13:21.524 [6781] main/110/lua I> wal_max_size = 268435456
2023-05-01 13:13:21.524 [6781] main/110/lua I> wal_queue_max_size = 16777216
2023-05-01 13:13:21.524 [6781] main/110/lua I> wal_cleanup_delay = 14400
2023-05-01 13:13:21.524 [6781] main/110/lua I> wal_mode = write
2023-05-01 13:13:21.524 [6781] main/110/lua I> work_dir = nil
2023-05-01 13:13:21.524 [6781] main/110/lua I> worker_pool_threads = 4
2023-05-01 13:13:21.524 [6781] main/110/lua I> txn_timeout = 3153600000
2023-05-01 13:13:21.524 [6781] main/110/lua I> Instance state changed: BoxConfigured -> ConfiguringRoles
2023-05-01 13:13:21.524 [6781] main/110/lua I> leaving orphan mode
2023-05-01 13:13:21.524 [6781] main/110/lua systemd.c:134 !> systemd: failed to send message: Transport endpoint is not connected
2023-05-01 13:13:21.524 [6781] main/110/lua I> set 'replication_connect_quorum' configuration option to 0
2023-05-01 13:13:21.524 [6781] main/110/lua I> Stateful failover enabled with stateboard at localhost:4401
2023-05-01 13:13:21.526 [6781] main/110/lua I> Not enough enabled instances to start leader_autoreturn fiber
2023-05-01 13:13:21.526 [6781] main/110/lua I> Replicaset 3a9bf4d4-1245-4830-a28d-ac1f1a8ab18e: new leader cdcc0521-d2a4-481d-9563-c2417e8ea53b ("localhost:3304"), was nil
2023-05-01 13:13:21.526 [6781] main/110/lua I> Replicaset 2a4b8056-5294-4836-b656-1cdc0cfc25a9: new leader ab53154e-78f8-4050-9b28-8ae0471d76aa ("localhost:3306"), was nil
2023-05-01 13:13:21.526 [6781] main/110/lua I> Replicaset a3be65c0-2696-4a2e-8b93-5acc710fcc06: new leader 9426b45c-8d9e-4076-a6d6-b5833e772ebf ("localhost:3303"), was nil
2023-05-01 13:13:21.526 [6781] main/110/lua I> Replicaset 23dbaca1-c88a-4fd5-953c-938a7c7c03d5 (me): new leader 8a1e23de-f0e4-43bd-8c9d-8933177a48cc (me), was nil
2023-05-01 13:13:21.526 [6781] main/110/lua I> Replicaset 56dd6a09-9960-48a9-8167-51999c47b192: new leader e988aa79-5982-4aa0-812e-6416c12e5844 ("localhost:3302"), was nil
2023-05-01 13:13:21.526 [6781] main/110/lua I> Replicaset 7b9da50b-4486-4033-b371-3a7ca6c570d6: new leader b80c2183-00eb-42f0-aeb9-f07ba66a561e ("localhost:3305"), was nil
2023-05-01 13:13:21.526 [6781] main/110/lua I> set 'read_only' configuration option to false
2023-05-01 13:13:21.529 [6781] main/110/lua I> Started queue state fiber
2023-05-01 13:13:21.529 [6781] main/110/lua I> Start applying roles config
2023-05-01 13:13:21.529 [6781] main/110/lua I> Init "ddl-manager" role
2023-05-01 13:13:21.529 [6781] main/110/lua I> Successfully initialized "ddl-manager" role in 0.000002 sec
2023-05-01 13:13:21.529 [6781] main/110/lua I> Appling "ddl-manager" role config
2023-05-01 13:13:21.529 [6781] main/110/lua I> Successfully applied "ddl-manager" role config in 0.000022 sec
2023-05-01 13:13:21.529 [6781] main/110/lua I> Init "vshard-router" role
2023-05-01 13:13:21.529 [6781] main/110/lua I> Successfully initialized "vshard-router" role in 0.000007 sec
2023-05-01 13:13:21.529 [6781] main/110/lua I> Appling "vshard-router" role config
2023-05-01 13:13:21.529 [6781] main/110/lua I> Reconfiguring vshard-router/default ...
2023-05-01 13:13:21.529 [6781] main/110/lua I> Starting router configuration
2023-05-01 13:13:21.529 [6781] main/110/lua I> Calling box.cfg()...
2023-05-01 13:13:21.529 [6781] main/110/lua I> {"read_only":false}
2023-05-01 13:13:21.529 [6781] main/110/lua I> Box has been configured
2023-05-01 13:13:21.531 [6781] main/132/localhost:3305 (net.box) net_box.lua:342 W> localhost:3305: unexpected EOF when reading from socket, called on fd 28, aka 127.0.0.1:53257, peer of 127.0.0.1:3305: Broken pipe
2023-05-01 13:13:21.537 [6781] main/133/localhost:3304 (net.box) net_box.lua:342 W> localhost:3304: unexpected EOF when reading from socket, called on fd 27, aka 127.0.0.1:53258, peer of 127.0.0.1:3304: Broken pipe
2023-05-01 13:13:21.537 [6781] main/135/localhost:3303 (net.box) net_box.lua:342 W> localhost:3303: unexpected EOF when reading from socket, called on fd 30, aka 127.0.0.1:53260, peer of 127.0.0.1:3303: Broken pipe
2023-05-01 13:13:21.539 [6781] main/134/localhost:3302 (net.box) net_box.lua:342 W> localhost:3302: unexpected EOF when reading from socket, called on fd 29, aka 127.0.0.1:53259, peer of 127.0.0.1:3302: Broken pipe
2023-05-01 13:13:22.034 [6781] main/132/localhost:3305 (net.box) I> connected to localhost:3305
2023-05-01 13:13:22.038 [6781] main/133/localhost:3304 (net.box) I> connected to localhost:3304
2023-05-01 13:13:22.039 [6781] main/135/localhost:3303 (net.box) I> connected to localhost:3303
2023-05-01 13:13:22.040 [6781] main/134/localhost:3302 (net.box) I> connected to localhost:3302
2023-05-01 13:13:22.041 [6781] main/139/vshard.failover._static_router I> failover_f has been started
2023-05-01 13:13:22.041 [6781] main/139/vshard.failover._static_router I> New replica localhost:3305(admin@localhost:3305) for replicaset(uuid="7b9da50b-4486-4033-b371-3a7ca6c570d6", master=localhost:3305(admin@localhost:3305))
2023-05-01 13:13:22.041 [6781] main/139/vshard.failover._static_router I> New replica localhost:3304(admin@localhost:3304) for replicaset(uuid="3a9bf4d4-1245-4830-a28d-ac1f1a8ab18e", master=localhost:3304(admin@localhost:3304))
2023-05-01 13:13:22.041 [6781] main/139/vshard.failover._static_router I> New replica localhost:3302(admin@localhost:3302) for replicaset(uuid="56dd6a09-9960-48a9-8167-51999c47b192", master=localhost:3302(admin@localhost:3302))
2023-05-01 13:13:22.041 [6781] main/139/vshard.failover._static_router I> New replica localhost:3303(admin@localhost:3303) for replicaset(uuid="a3be65c0-2696-4a2e-8b93-5acc710fcc06", master=localhost:3303(admin@localhost:3303))
2023-05-01 13:13:22.041 [6781] main/139/vshard.failover._static_router I> All replicas are ok
2023-05-01 13:13:22.041 [6781] main/139/vshard.failover._static_router I> Failovering step is finished. Schedule next after 1.000000 seconds
2023-05-01 13:13:22.041 [6781] main/141/vshard.discovery._static_router I> discovery_f has been started
2023-05-01 13:13:22.041 [6781] main/110/lua I> Successfully applied "vshard-router" role config in 0.512046 sec
2023-05-01 13:13:22.041 [6781] main/110/lua I> Init "metrics" role
2023-05-01 13:13:22.046 [6781] main/110/lua I> Successfully initialized "metrics" role in 0.004756 sec
2023-05-01 13:13:22.046 [6781] main/110/lua I> Appling "metrics" role config
2023-05-01 13:13:22.046 [6781] main/110/lua I> Successfully applied "metrics" role config in 0.000515 sec
2023-05-01 13:13:22.046 [6781] main/110/lua I> Init "crud-router" role
2023-05-01 13:13:22.046 [6781] main/110/lua I> Successfully initialized "crud-router" role in 0.000007 sec
2023-05-01 13:13:22.046 [6781] main/110/lua I> Appling "crud-router" role config
2023-05-01 13:13:22.046 [6781] main/110/lua I> Successfully applied "crud-router" role config in 0.000019 sec
2023-05-01 13:13:22.046 [6781] main/110/lua I> Init "api" role
2023-05-01 13:13:22.047 [6781] main/110/lua I> Successfully initialized "api" role in 0.000099 sec
2023-05-01 13:13:22.047 [6781] main/110/lua I> Init "repo_api" role
2023-05-01 13:13:22.047 [6781] main/110/lua I> Successfully initialized "repo_api" role in 0.000103 sec
2023-05-01 13:13:22.047 [6781] main/110/lua I> Roles configuration finished
2023-05-01 13:13:22.047 [6781] main/110/lua I> Instance state changed: ConfiguringRoles -> RolesConfigured
2023-05-01 13:13:22.084 [6781] main/141/vshard.discovery._static_router I> Updated replicaset(uuid="a3be65c0-2696-4a2e-8b93-5acc710fcc06", master=localhost:3303(admin@localhost:3303)) buckets: was 0, became 1000
2023-05-01 13:13:22.084 [6781] main/141/vshard.discovery._static_router I> Updated replicaset(uuid="3a9bf4d4-1245-4830-a28d-ac1f1a8ab18e", master=localhost:3304(admin@localhost:3304)) buckets: was 0, became 1000
2023-05-01 13:13:22.084 [6781] main/141/vshard.discovery._static_router I> Updated replicaset(uuid="56dd6a09-9960-48a9-8167-51999c47b192", master=localhost:3302(admin@localhost:3302)) buckets: was 0, became 1000
2023-05-01 13:13:22.084 [6781] main/141/vshard.discovery._static_router I> Updated replicaset(uuid="7b9da50b-4486-4033-b371-3a7ca6c570d6", master=localhost:3305(admin@localhost:3305)) buckets: was 0, became 1000
2023-05-01 13:13:22.084 [6781] main/141/vshard.discovery._static_router I> Start aggressive discovery, 26000 buckets are unknown. Discovery works with 1 seconds interval
2023-05-01 13:13:23.042 [6781] main/139/vshard.failover._static_router I> All replicas are ok
2023-05-01 13:13:23.130 [6781] main/141/vshard.discovery._static_router I> Updated replicaset(uuid="a3be65c0-2696-4a2e-8b93-5acc710fcc06", master=localhost:3303(admin@localhost:3303)) buckets: was 1000, became 2000
2023-05-01 13:13:23.130 [6781] main/141/vshard.discovery._static_router I> Updated replicaset(uuid="3a9bf4d4-1245-4830-a28d-ac1f1a8ab18e", master=localhost:3304(admin@localhost:3304)) buckets: was 1000, became 2000
2023-05-01 13:13:23.130 [6781] main/141/vshard.discovery._static_router I> Updated replicaset(uuid="56dd6a09-9960-48a9-8167-51999c47b192", master=localhost:3302(admin@localhost:3302)) buckets: was 1000, became 2000
2023-05-01 13:13:23.130 [6781] main/141/vshard.discovery._static_router I> Updated replicaset(uuid="7b9da50b-4486-4033-b371-3a7ca6c570d6", master=localhost:3305(admin@localhost:3305)) buckets: was 1000, became 2000
2023-05-01 13:13:23.573 [6781] main C> got signal 15 - Terminated
2023-05-01 13:13:23.574 [6781] main/156/trigger_fiber1 I> Stop "ddl-manager" role
2023-05-01 13:13:23.574 [6781] main/156/trigger_fiber1 I> Successfully stopped "ddl-manager" role in 0.000003 sec
2023-05-01 13:13:23.574 [6781] main/156/trigger_fiber1 I> Stop "vshard-router" role
2023-05-01 13:13:23.574 [6781] main/156/trigger_fiber1 I> Starting router configuration
2023-05-01 13:13:23.574 [6781] main/156/trigger_fiber1 I> Calling box.cfg()...
2023-05-01 13:13:23.574 [6781] main/156/trigger_fiber1 I> Box has been configured
2023-05-01 13:13:23.574 [6781] main/158/lua I> Old replicaset and replica objects are outdated.
2023-05-01 13:13:23.574 [6781] main/156/trigger_fiber1 I> Successfully stopped "vshard-router" role in 0.000664 sec
2023-05-01 13:13:23.574 [6781] main/156/trigger_fiber1 I> Stop "metrics" role
2023-05-01 13:13:23.574 [6781] main/156/trigger_fiber1 I> Successfully stopped "metrics" role in 0.000010 sec
2023-05-01 13:13:23.574 [6781] main/156/trigger_fiber1 I> Stop "crud-router" role
2023-05-01 13:13:23.574 [6781] main/156/trigger_fiber1 I> Successfully stopped "crud-router" role in 0.000002 sec
2023-05-01 13:13:23.575 [6781] main/155/iproto.shutdown I> tx_binary: stopped
